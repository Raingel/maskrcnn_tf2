{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-RCNN inference with tensorflow, onnxruntime, TensorRT engine.  Balloon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import onnx_graphsurgeon as gs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers import losses\n",
    "from training import get_optimizer\n",
    "from model import mask_rcnn_functional\n",
    "from common import inference_utils\n",
    "from common.inference_utils import process_input\n",
    "from common import utils\n",
    "from common.config import CONFIG\n",
    "\n",
    "from common.inference_optimize import maskrcnn_to_onnx, modify_onnx_model\n",
    "\n",
    "import tensorflow as tf\n",
    "utils.tf_limit_gpu_memory(tf, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'maskrcnn_seresnet34_14735ea1954396a749b4de160c9ce5c8_cp-0050.ckpt'\n",
    "weights_path = os.path.join('..', 'tests', 'samples', 'balloon', checkpoint)\n",
    "weights_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading inference graph and import weights\n",
    "\n",
    "CONFIG.update({'class_dict': {'balloon': 1, 'background': 0},\n",
    "               'num_classes': 2,\n",
    "               'backbone': 'seresnet34',\n",
    "              },\n",
    "             )\n",
    "CONFIG.update({'meta_shape': (1 + 3 + 3 + 4 + 1 + CONFIG['num_classes']),})\n",
    "model_name = f\"\"\"maskrcnn_{CONFIG['backbone']}_{'_'.join(list(map(str, CONFIG['image_shape'])))}\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = CONFIG\n",
    "inference_config.update({'training': False})\n",
    "inference_model = mask_rcnn_functional(config=inference_config)\n",
    "inference_model = inference_utils.load_mrcnn_weights(model=inference_model,\n",
    "                                                     weights_path=weights_path,\n",
    "                                                     verbose=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Run several tests with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = '../tests/images/balloon'\n",
    "os.listdir(test_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "    output = inference_model([np.expand_dims(img_processed, 0),\n",
    "                              np.expand_dims(image_meta, 0)]\n",
    "                            ) \n",
    "    \n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "    \n",
    "    print(img_name, '\\nOutput shapes:')\n",
    "    for out in output:\n",
    "        print(out.shape)\n",
    "    \n",
    "    \n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=detections[0].numpy(), \n",
    "                              mrcnn_mask=mrcnn_mask[0].numpy(), \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "    plt.imshow(img_show)\n",
    "    \n",
    "    for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "        fig=plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "        plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert model to .onnx with tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spec = (\n",
    "    tf.TensorSpec((CONFIG['batch_size'], *CONFIG['image_shape']), tf.float32, name=\"input_image\"),\n",
    "    tf.TensorSpec((CONFIG['batch_size'], CONFIG['meta_shape']), tf.float32, name=\"input_image_meta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn_to_onnx(model=inference_model, \n",
    "                 model_name = model_name,\n",
    "                 input_spec=input_spec,\n",
    "                 kwargs={'opset': 11}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load onnx model and check it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3.onnx\"\"\")\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run several tests with onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = ort.InferenceSession(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3.onnx\"\"\")\n",
    "print(f'Inputs: {[x.name for x in sess.get_inputs()]}\\nOutputs:{[x.name for x in sess.get_outputs()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "    try:\n",
    "        output = sess.run(output_names=[x.name for x in sess.get_outputs()], \n",
    "                          input_feed={'input_image': np.expand_dims(img_processed, 0).astype('float32'),\n",
    "                                      'input_image_meta': np.expand_dims(image_meta, 0).astype('float32'),\n",
    "                                     }\n",
    "                         )\n",
    "\n",
    "        detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "\n",
    "        print(img_name, '\\nOutput shapes:')\n",
    "        for out in output:\n",
    "            print(out.shape)\n",
    "\n",
    "\n",
    "        # Extract bboxes, class_ids, scores and full-size masks\n",
    "        boxes, class_ids, scores, full_masks = \\\n",
    "        utils.reformat_detections(detections=detections[0], \n",
    "                                  mrcnn_mask=mrcnn_mask[0], \n",
    "                                  original_image_shape=img.shape, \n",
    "                                  image_shape=img_processed.shape, \n",
    "                                  window=window\n",
    "                                 )\n",
    "\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        plt.title('Input data')\n",
    "        plt.imshow(img_show)\n",
    "\n",
    "        for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "            fig=plt.figure(figsize=(5,5))\n",
    "            plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "            plt.imshow(fm)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model for TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modify_onnx_model(model_path=f'../weights/{model_name}.onnx',\n",
    "                  config=CONFIG,\n",
    "                  verbose=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorRT optimization\n",
    "\n",
    "__With trtexec:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.chdir('../weights')\n",
    "\n",
    "# Construct appropriate command\n",
    "fp16_mode = False\n",
    "\n",
    "command = [os.environ['TRTEXEC'],\n",
    "           f'--onnx={model_name}_trt_mod.onnx',\n",
    "           f'--saveEngine={model_name}_trt_mod_fp32.engine',\n",
    "            '--workspace=2048',\n",
    "            '--explicitBatch',\n",
    "            '--verbose',\n",
    "          ]\n",
    "\n",
    "# fp16 param\n",
    "if fp16_mode:\n",
    "    command[2].replace('32', '16')\n",
    "    command.append('--fp16')\n",
    "\n",
    "# tacticSources param\n",
    "# Do not neeed on jetson with aarch64 architecture for now.\n",
    "arch = os.uname().machine\n",
    "if arch == 'x86_64':\n",
    "    command.append('--tacticSources=-cublasLt,+cublas')\n",
    "    \n",
    "print(f'\\nArch: {arch}\\ntrtexec command list: {command}')\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, check=True)\n",
    "# Print stdout inference result\n",
    "print(result.stdout.decode('utf8')[-2495:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__With python TensorRT API:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 1\n",
    "# Precision mode\n",
    "fp16_mode = True\n",
    "# Workspace size in Mb\n",
    "wspace_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Init TensorRT Logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "# Init TensorRT plugins\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "# Set tensorrt-prepared onnx model\n",
    "onnx_model_path = f'../weights/{model_name}_trt_mod.onnx' \n",
    "\n",
    "# Use explicit batch\n",
    "explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "        builder.create_builder_config() as builder_config, \\\n",
    "        builder.create_network(explicit_batch) as network, \\\n",
    "        trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "    with open(onnx_model_path, 'rb') as model:\n",
    "        parser.parse(model.read())\n",
    "\n",
    "    print('Num of detected layers: ', network.num_layers)\n",
    "    print('Detected inputs: ', network.num_inputs)\n",
    "    print('Detected outputs: ', network.num_outputs)\n",
    "    \n",
    "    # Workspace size\n",
    "    # 1e6 bytes == 1Mb\n",
    "    builder_config.max_workspace_size = int(1e6 * wspace_size)\n",
    "    \n",
    "    # Precision mode\n",
    "    if fp16_mode:\n",
    "        builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "    \n",
    "    # Max batch size\n",
    "    builder.max_batch_size = max_batch_size\n",
    "    \n",
    "    # Set the list of tactic sources\n",
    "    # Do not need for Jetson with aarch64 architecture for now\n",
    "    arch = os.uname().machine\n",
    "    if arch == 'x86_64':\n",
    "        tactic_source = 1 << int(trt.TacticSource.CUBLAS) | 0 << int(trt.TacticSource.CUBLAS_LT)\n",
    "        builder_config.set_tactic_sources(tactic_source)\n",
    "        \n",
    "    \n",
    "    # Make TensorRT engine\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "    \n",
    "    # Save TensorRT engine\n",
    "    if fp16_mode:\n",
    "        trt_model_name = f'../weights/{model_name}_trt_mod_fp16.engine'\n",
    "        \n",
    "    else:\n",
    "        trt_model_name = f'../weights/{model_name}_trt_mod_fp32_trt.engine'\n",
    "\n",
    "    with open(trt_model_name, \"wb\") as f:\n",
    "        f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run TensorRT inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_mrcnn_inference(model, image):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        image: prepared image for inference\n",
    "\n",
    "    Returns: boxes,\n",
    "             class_ids, \n",
    "             scores, f\n",
    "             ull_masks, \n",
    "             eval_gt_boxes, \n",
    "             eval_gt_class_ids, \n",
    "             eval_gt_masks\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract trt-variables from a dict for transparency\n",
    "    engine = model['engine']\n",
    "    stream = model['stream']\n",
    "    context = model['context']\n",
    "    device_input = model['device_input']\n",
    "    device_output1 = model['device_output1']\n",
    "    device_output2 = model['device_output2']\n",
    "\n",
    "    host_output1 = model['host_output1']\n",
    "    host_output2 = model['host_output2']\n",
    "\n",
    "    # Make inference\n",
    "    host_input = image.astype(dtype=np.float32, order='C')\n",
    "    cuda.memcpy_htod_async(device_input, host_input, stream)\n",
    "    context.execute_async(bindings=[int(device_input),\n",
    "                                    int(device_output1),\n",
    "                                    int(device_output2),\n",
    "                                    ],\n",
    "                          stream_handle=stream.handle)\n",
    "\n",
    "    cuda.memcpy_dtoh_async(host_output1, device_output1, stream)\n",
    "    cuda.memcpy_dtoh_async(host_output2, device_output2, stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    trt_mrcnn_detection = host_output1.reshape(\n",
    "        engine.get_binding_shape('mrcnn_detection')).astype(dtype=np.float32)\n",
    "    trt_mrcnn_mask = host_output2.reshape(\n",
    "        engine.get_binding_shape('mrcnn_mask')).astype(dtype=np.float32)\n",
    "    \n",
    "    return trt_mrcnn_detection, trt_mrcnn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mrcnn_trt_engine(model_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load TensorRT engine via pycuda\n",
    "    Args:\n",
    "        model_path: model path to TensorRT-engine\n",
    "\n",
    "    Returns: python dict of attributes for pycuda model inference\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trt_logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "    trt.init_libnvinfer_plugins(trt_logger, \"\")\n",
    "\n",
    "    with open(model_path, \"rb\") as f, trt.Runtime(trt_logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Inputs\n",
    "    input_shape = engine.get_binding_shape('input_image')\n",
    "    input_size = trt.volume(input_shape) *\\\n",
    "                 engine.max_batch_size * np.dtype(np.float32).itemsize\n",
    "    device_input = cuda.mem_alloc(input_size)\n",
    "\n",
    "    # Outputs\n",
    "    output_names = list(engine)[1:]\n",
    "\n",
    "    # mrcnn_detection output\n",
    "    output_shape1 = engine.get_binding_shape('mrcnn_detection')\n",
    "    host_output1 = cuda.pagelocked_empty(trt.volume(output_shape1) *\n",
    "                                              engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output1 = cuda.mem_alloc(host_output1.nbytes)\n",
    "\n",
    "\n",
    "    # mrcnn_mask output\n",
    "    output_shape2 = engine.get_binding_shape('mrcnn_mask')\n",
    "    host_output2 = cuda.pagelocked_empty(trt.volume(output_shape2) * engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output2 = cuda.mem_alloc(host_output2.nbytes)\n",
    "\n",
    "    # Setting a cuda stream\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    return {'engine': engine,\n",
    "            'stream': stream,\n",
    "            'context': context,\n",
    "            'device_input': device_input,\n",
    "            'device_output1': device_output1,\n",
    "            'device_output2':device_output2,\n",
    "            'host_output1': host_output1,\n",
    "            'host_output2': host_output2\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = set_mrcnn_trt_engine(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3_trt_mod_fp32.engine\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "\n",
    "    trt_mrcnn_detection, trt_mrcnn_mask = trt_mrcnn_inference(trt_model, np.expand_dims(img_processed, 0))\n",
    "    \n",
    "\n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=trt_mrcnn_detection[0], \n",
    "                              mrcnn_mask=trt_mrcnn_mask[0], \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "    plt.imshow(img_show)\n",
    "\n",
    "    for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "        fig=plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "        plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
