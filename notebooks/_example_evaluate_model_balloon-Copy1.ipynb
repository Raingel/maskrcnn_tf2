{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-RCNN evaluate model. Balloon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir('..')\n",
    "\n",
    "from samples.balloon import balloon\n",
    "from preprocess import preprocess\n",
    "from preprocess import augmentation as aug\n",
    "\n",
    "from model import mask_rcnn_functional\n",
    "import evaluating\n",
    "from common import utils\n",
    "from common import inference_utils\n",
    "from common.inference_utils import process_input\n",
    "from common.config import CONFIG\n",
    "from common.inference_optimize import maskrcnn_to_onnx, modify_onnx_model\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "utils.tf_limit_gpu_memory(tf, 2000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd().replace('src', 'balloon')\n",
    "eval_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.config import CONFIG\n",
    "\n",
    "CONFIG.update(balloon.BALLOON_CONFIG)\n",
    "CONFIG.update({'class_dict': {'balloon': 1, 'background': 0},\n",
    "               'num_classes': 2,\n",
    "               'backbone': 'seresnet34',\n",
    "              },\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = balloon.BalloonDataset(images_dir=eval_dir,\n",
    "                                     class_key='object',\n",
    "                                     classes_dict=CONFIG['class_dict'],\n",
    "                                     preprocess_transform=preprocess.get_input_preprocess(\n",
    "                                         normalize=CONFIG['normalization']\n",
    "                                     ),\n",
    "                                     json_annotation_key=None,\n",
    "                                     **CONFIG\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = preprocess.DataLoader(eval_dataset,\n",
    "                                        shuffle=True,\n",
    "                                        cast_output=False,\n",
    "                                        return_original=True,\n",
    "                                         **CONFIG\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'maskrcnn_seresnet34_14735ea1954396a749b4de160c9ce5c8_cp-0050.ckpt'\n",
    "weights_path = os.path.join('..', 'tests', 'samples', 'balloon', checkpoint)\n",
    "weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading inference graph and import weights\n",
    "inference_config = CONFIG\n",
    "inference_config.update({'training': False})\n",
    "inference_model = mask_rcnn_functional(config=inference_config)\n",
    "inference_model = inference_utils.load_mrcnn_weights(model=inference_model,\n",
    "                                                     weights_path=weights_path,\n",
    "                                                     verbose=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate data on a single batch with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mrcnn_inference(model, infer_batch, eval_batch):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        infer_batch: prepared data for inference\n",
    "        eval_batch:  ground truth data for evaluation\n",
    "\n",
    "    Returns: boxes,\n",
    "             class_ids, \n",
    "             scores, \n",
    "             ull_masks, \n",
    "             eval_gt_boxes, \n",
    "             eval_gt_class_ids, \n",
    "             eval_gt_masks\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract inference inputs from dataloader\n",
    "    batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox, \\\n",
    "    batch_gt_class_ids, batch_gt_boxes, batch_gt_masks = infer_batch\n",
    "\n",
    "    # Extract original inputs from dataloader\n",
    "    eval_gt_image = eval_batch[0][0]\n",
    "    eval_gt_boxes = eval_batch[3][0]\n",
    "    eval_gt_class_ids = eval_batch[2][0]\n",
    "    eval_gt_masks = eval_batch[1][0]\n",
    "    \n",
    "    # Make inference\n",
    "    output = model([batch_images, batch_image_meta])\n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "\n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "        utils.reformat_detections(detections=detections[0].numpy(),\n",
    "                                  mrcnn_mask=mrcnn_mask[0].numpy(),\n",
    "                                  original_image_shape=eval_gt_image.shape,\n",
    "                                  image_shape=batch_images[0].shape,\n",
    "                                  window=batch_image_meta[0][7:11]\n",
    "                                  )\n",
    "    return boxes, class_ids, scores, full_masks, eval_gt_boxes, eval_gt_class_ids, eval_gt_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_mrcnn(model, inference_function, eval_dataloader, iou_limits=(0.5, 1), iou_step=0.05):\n",
    "    \"\"\"\n",
    "    Evaluate Mask-RCNN model\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        inference_function:\n",
    "        eval_dataloader:\n",
    "        iou_limits: start and end for IoU in mAP\n",
    "        iou_step:   step for IoU in mAP\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Evaluate mAP\n",
    "    for eval_iou_threshold in np.arange(iou_limits[0], iou_limits[1], iou_step):\n",
    "\n",
    "        # Metrics lists\n",
    "        ap_list = []\n",
    "        precisions_list = []\n",
    "        recalls_list = []\n",
    "\n",
    "        eval_iterated = iter(eval_dataloader)\n",
    "        pbar = tqdm.tqdm(eval_iterated, total=eval_dataloader.__len__())\n",
    "\n",
    "        for eval_input, _ in pbar:\n",
    "            # Split batch into prepared data for inference and original data for evaluation\n",
    "            infer_batch = eval_input[:-4]\n",
    "            eval_batch = eval_input[-4:]\n",
    "            \n",
    "            try:\n",
    "                boxes, class_ids, scores, full_masks, eval_gt_boxes, eval_gt_class_ids, eval_gt_masks = \\\n",
    "                    inference_function(model=model, infer_batch=infer_batch, eval_batch=eval_batch)\n",
    "\n",
    "                # Get AP, precisions, recalls, overlaps\n",
    "                ap, precisions, recalls, overlaps = \\\n",
    "                    evaluating.compute_ap(gt_boxes=eval_gt_boxes,\n",
    "                                          gt_class_ids=eval_gt_class_ids,\n",
    "                                          gt_masks=eval_gt_masks,\n",
    "                                          pred_boxes=boxes,\n",
    "                                          pred_class_ids=class_ids,\n",
    "                                          pred_scores=scores,\n",
    "                                          pred_masks=full_masks,\n",
    "                                          iou_threshold=eval_iou_threshold\n",
    "                                          )\n",
    "                postfix = ''\n",
    "            except:\n",
    "                postfix = 'Passed an image. AP added as zero.'\n",
    "                ap = 0.0\n",
    "                precisions = 0.0\n",
    "                recalls = 0.0\n",
    "            \n",
    "            ap_list.append(ap)\n",
    "            precisions_list.append(precisions)\n",
    "            recalls_list.append(recalls)\n",
    "\n",
    "            # Update tqdm mAP\n",
    "            pbar.set_description(f\"IoU: {eval_iou_threshold:.2f}. mAP: {np.mean(ap_list):.4f} \")# {postfix}\n",
    "\n",
    "\n",
    "        print(f'mAP={np.mean(ap_list):.4f}, IoU: {eval_iou_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_mrcnn(model=inference_model,\n",
    "               inference_function=tf_mrcnn_inference,\n",
    "               eval_dataloader=eval_dataloader\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate data on a single batch with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_mrcnn_inference(model, infer_batch, eval_batch):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        infer_batch: prepared data for inference\n",
    "        eval_batch:  ground truth data for evaluation\n",
    "\n",
    "    Returns: boxes,\n",
    "             class_ids, \n",
    "             scores, f\n",
    "             ull_masks, \n",
    "             eval_gt_boxes, \n",
    "             eval_gt_class_ids, \n",
    "             eval_gt_masks\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract inference inputs from dataloader\n",
    "    batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox, \\\n",
    "    batch_gt_class_ids, batch_gt_boxes, batch_gt_masks = infer_batch\n",
    "\n",
    "    # Extract original inputs from dataloader\n",
    "    eval_gt_image = eval_batch[0][0]\n",
    "    eval_gt_boxes = eval_batch[3][0]\n",
    "    eval_gt_class_ids = eval_batch[2][0]\n",
    "    eval_gt_masks = eval_batch[1][0]\n",
    "\n",
    "    # Extract trt-variables from a dict for transparency\n",
    "    engine = model['engine']\n",
    "    stream = model['stream']\n",
    "    context = model['context']\n",
    "    device_input = model['device_input']\n",
    "    device_output1 = model['device_output1']\n",
    "    device_output2 = model['device_output2']\n",
    "\n",
    "    host_output1 = model['host_output1']\n",
    "    host_output2 = model['host_output2']\n",
    "    \n",
    "    output_nodes = model['output_nodes']\n",
    "    graph_type = model['graph_type']\n",
    "    \n",
    "    \n",
    "    if graph_type == 'uff':\n",
    "        # Prepare image for uff original graph\n",
    "        input_image, window, scale, padding, crop = utils.resize_image(\n",
    "                eval_gt_image,\n",
    "                min_dim=800,\n",
    "                min_scale=0,\n",
    "                max_dim=1024,\n",
    "                mode='square')\n",
    "        #  Substract channel-mean\n",
    "        input_image = input_image.astype(np.float32) - np.array([123.7, 116.8, 103.9])\n",
    "        \n",
    "        image_shape_reformat = input_image.shape\n",
    "        \n",
    "        # Add batch dimension\n",
    "        batch_images = np.expand_dims(input_image, 0)\n",
    "        # (batch, w, h, 3) -> (batch, 3, w, h)\n",
    "        batch_images = np.moveaxis(batch_images, -1, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        window = batch_image_meta[0][7:11]\n",
    "        image_shape_reformat = batch_images[0].shape\n",
    "\n",
    "    # Make inference\n",
    "    host_input = batch_images.astype(dtype=np.float32, order='C')\n",
    "    cuda.memcpy_htod_async(device_input, host_input, stream)\n",
    "    context.execute_async(bindings=[int(device_input),\n",
    "                                    int(device_output1),\n",
    "                                    int(device_output2),\n",
    "                                    ],\n",
    "                          stream_handle=stream.handle)\n",
    "\n",
    "    cuda.memcpy_dtoh_async(host_output1, device_output1, stream)\n",
    "    cuda.memcpy_dtoh_async(host_output2, device_output2, stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    output_shape1 = engine.get_binding_shape(output_nodes[0])\n",
    "    output_shape2 = engine.get_binding_shape(output_nodes[1])\n",
    "    \n",
    "    if graph_type == 'onnx':\n",
    "        trt_mrcnn_detection = host_output1.reshape(output_shape1).astype(dtype=np.float32)\n",
    "        trt_mrcnn_mask = host_output2.reshape(output_shape2).astype(dtype=np.float32)\n",
    "    elif graph_type == 'uff':\n",
    "        # (batch, 100, 6)\n",
    "        trt_mrcnn_detection = host_output1.reshape(\n",
    "            (engine.max_batch_size, *output_shape1)).astype(dtype=np.float32)\n",
    "        # (batch, 100, 2, 28, 28)\n",
    "        trt_mrcnn_mask = host_output2.reshape(\n",
    "            (engine.max_batch_size, *output_shape2)).astype(dtype=np.float32)\n",
    "        # (batch, 100, 2, 28, 28) -> (batch, 100, 28, 28, 2)\n",
    "        trt_mrcnn_mask = np.moveaxis(trt_mrcnn_mask, 2, -1)\n",
    "    else:\n",
    "        raise ValueError(f'Only onnx and uff graph types. Passed: {graph_type}')\n",
    "        \n",
    "\n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    trt_boxes, trt_class_ids, trt_scores, trt_full_masks = \\\n",
    "        utils.reformat_detections(detections=trt_mrcnn_detection[0],\n",
    "                                  mrcnn_mask=trt_mrcnn_mask[0],\n",
    "                                  original_image_shape=eval_gt_image.shape,\n",
    "                                  image_shape=image_shape_reformat,\n",
    "                                  window=window\n",
    "                                  )\n",
    "    \n",
    "    return trt_boxes, trt_class_ids, trt_scores, trt_full_masks, eval_gt_boxes, eval_gt_class_ids, eval_gt_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mrcnn_trt_engine(model_path, output_nodes=['mrcnn_detection', 'mrcnn_mask'], graph_type='onnx'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load TensorRT engine via pycuda\n",
    "    Args:\n",
    "        model_path: model path to TensorRT-engine\n",
    "        output_nodes: output nodes names\n",
    "        graph_type: onnx or uff\n",
    "\n",
    "    Returns: python dict of attributes for pycuda model inference\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trt_logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "    trt.init_libnvinfer_plugins(trt_logger, \"\")\n",
    "\n",
    "    with open(model_path, \"rb\") as f, trt.Runtime(trt_logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Inputs\n",
    "    input_shape = engine.get_binding_shape('input_image')\n",
    "    input_size = trt.volume(input_shape) *\\\n",
    "                 engine.max_batch_size * np.dtype(np.float32).itemsize\n",
    "    device_input = cuda.mem_alloc(input_size)\n",
    "\n",
    "    # Outputs\n",
    "    output_names = list(engine)[1:]\n",
    "\n",
    "    # mrcnn_detection output\n",
    "    output_shape1 = engine.get_binding_shape(output_nodes[0])\n",
    "    host_output1 = cuda.pagelocked_empty(trt.volume(output_shape1) *\n",
    "                                              engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output1 = cuda.mem_alloc(host_output1.nbytes)\n",
    "\n",
    "\n",
    "    # mrcnn_mask output\n",
    "    output_shape2 = engine.get_binding_shape(output_nodes[1])\n",
    "    host_output2 = cuda.pagelocked_empty(trt.volume(output_shape2) * engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output2 = cuda.mem_alloc(host_output2.nbytes)\n",
    "\n",
    "    # Setting a cuda stream\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    return {'engine': engine,\n",
    "            'stream': stream,\n",
    "            'context': context,\n",
    "            'device_input': device_input,\n",
    "            'device_output1': device_output1,\n",
    "            'device_output2':device_output2,\n",
    "            'host_output1': host_output1,\n",
    "            'host_output2': host_output2,\n",
    "            'output_nodes': output_nodes,\n",
    "            'graph_type': graph_type\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_mrcnn(model=set_mrcnn_trt_engine(\n",
    "    model_path=f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3_trt_mod_fp32.engine\"\"\"),\n",
    "               inference_function=trt_mrcnn_inference,\n",
    "               eval_dataloader=eval_dataloader\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mrcnn(model=set_mrcnn_trt_engine(\n",
    "    model_path=f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3_trt_mod_fp16.engine\"\"\"),\n",
    "               inference_function=trt_mrcnn_inference,\n",
    "               eval_dataloader=eval_dataloader\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
